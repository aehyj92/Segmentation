{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d243995",
   "metadata": {},
   "source": [
    "## json parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "0179a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('/workspace/task6/data_all.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57d90680",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Task_Name</th>\n",
       "      <th>Center</th>\n",
       "      <th>Image_no</th>\n",
       "      <th>Patient_no</th>\n",
       "      <th>Case_no</th>\n",
       "      <th>Slide_no</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Procedure</th>\n",
       "      <th>Location1</th>\n",
       "      <th>Location2</th>\n",
       "      <th>Invasiveness</th>\n",
       "      <th>LN meta</th>\n",
       "      <th>Distant meta</th>\n",
       "      <th>Pathologist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>F</td>\n",
       "      <td>Resection</td>\n",
       "      <td>HEAD_NECK</td>\n",
       "      <td>Face</td>\n",
       "      <td>Invasive</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>F</td>\n",
       "      <td>Resection</td>\n",
       "      <td>HEAD_NECK</td>\n",
       "      <td>Face</td>\n",
       "      <td>Invasive</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>F</td>\n",
       "      <td>Resection</td>\n",
       "      <td>HEAD_NECK</td>\n",
       "      <td>Face</td>\n",
       "      <td>Invasive</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>F</td>\n",
       "      <td>Resection</td>\n",
       "      <td>HEAD_NECK</td>\n",
       "      <td>Face</td>\n",
       "      <td>Invasive</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>F</td>\n",
       "      <td>Resection</td>\n",
       "      <td>HEAD_NECK</td>\n",
       "      <td>Face</td>\n",
       "      <td>Invasive</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3137</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1167</td>\n",
       "      <td>536</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>M</td>\n",
       "      <td>Biopsy</td>\n",
       "      <td>EXTREMITY</td>\n",
       "      <td>Sole</td>\n",
       "      <td>In situ</td>\n",
       "      <td>NS</td>\n",
       "      <td>No</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3138</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1168</td>\n",
       "      <td>537</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>F</td>\n",
       "      <td>Biopsy</td>\n",
       "      <td>EXTREMITY</td>\n",
       "      <td>Sole</td>\n",
       "      <td>In situ</td>\n",
       "      <td>NS</td>\n",
       "      <td>No</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3139</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1169</td>\n",
       "      <td>538</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>M</td>\n",
       "      <td>Biopsy</td>\n",
       "      <td>EXTREMITY</td>\n",
       "      <td>Sole</td>\n",
       "      <td>In situ</td>\n",
       "      <td>NS</td>\n",
       "      <td>No</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3140</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1170</td>\n",
       "      <td>539</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>M</td>\n",
       "      <td>Biopsy</td>\n",
       "      <td>EXTREMITY</td>\n",
       "      <td>Palm</td>\n",
       "      <td>In situ</td>\n",
       "      <td>NS</td>\n",
       "      <td>No</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3141</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1171</td>\n",
       "      <td>540</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>F</td>\n",
       "      <td>Biopsy</td>\n",
       "      <td>EXTREMITY</td>\n",
       "      <td>Sole</td>\n",
       "      <td>In situ</td>\n",
       "      <td>NS</td>\n",
       "      <td>No</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3142 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Task_Name  Center  Image_no  Patient_no  Case_no  Slide_no  Age Sex  \\\n",
       "0             6       1         1           1        1         1   60   F   \n",
       "1             6       1         2           1        1         2   60   F   \n",
       "2             6       1         3           1        1         3   60   F   \n",
       "3             6       1         4           1        1         4   60   F   \n",
       "4             6       1         5           1        1         5   60   F   \n",
       "...         ...     ...       ...         ...      ...       ...  ...  ..   \n",
       "3137          6       4      1167         536        1         1   62   M   \n",
       "3138          6       4      1168         537        1         1   69   F   \n",
       "3139          6       4      1169         538        1         1   65   M   \n",
       "3140          6       4      1170         539        1         1   44   M   \n",
       "3141          6       4      1171         540        1         1   48   F   \n",
       "\n",
       "      Procedure  Location1 Location2 Invasiveness LN meta Distant meta  \\\n",
       "0     Resection  HEAD_NECK      Face     Invasive     Yes           No   \n",
       "1     Resection  HEAD_NECK      Face     Invasive     Yes           No   \n",
       "2     Resection  HEAD_NECK      Face     Invasive     Yes           No   \n",
       "3     Resection  HEAD_NECK      Face     Invasive     Yes           No   \n",
       "4     Resection  HEAD_NECK      Face     Invasive     Yes           No   \n",
       "...         ...        ...       ...          ...     ...          ...   \n",
       "3137     Biopsy  EXTREMITY      Sole      In situ      NS           No   \n",
       "3138     Biopsy  EXTREMITY      Sole      In situ      NS           No   \n",
       "3139     Biopsy  EXTREMITY      Sole      In situ      NS           No   \n",
       "3140     Biopsy  EXTREMITY      Palm      In situ      NS           No   \n",
       "3141     Biopsy  EXTREMITY      Sole      In situ      NS           No   \n",
       "\n",
       "      Pathologist  \n",
       "0              11  \n",
       "1              11  \n",
       "2              11  \n",
       "3              11  \n",
       "4              11  \n",
       "...           ...  \n",
       "3137           11  \n",
       "3138           11  \n",
       "3139           11  \n",
       "3140           11  \n",
       "3141           11  \n",
       "\n",
       "[3142 rows x 15 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "base_path = '/workspace/task6/04_최종_xml'\n",
    "base_save_path = '/workspace/task6/04_최종_라벨링데이터'\n",
    "clinical_info_path = '/workspace/task6/1205_clinical_info.csv'\n",
    "df = pd.read_csv(clinical_info_path)\n",
    "# 결측값 처리1\n",
    "df = df.fillna(-9999999)\n",
    "# assert len(glob(base_path+'/*.xml')) == len(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2326d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########final 파싱 #########\n",
    "\n",
    "except_list=[]\n",
    "execpt_list_1=[]\n",
    "for row_ in df.iterrows():\n",
    "    case_dict = dict(row_[1])\n",
    "    case_dict = {k : None if case_dict[k] == -9999999 else v for k, v in case_dict.items()}\n",
    "    task_ = '{:02d}'.format(case_dict['Task_Name'])\n",
    "    center_ = '{:02d}'.format(case_dict['Center'])\n",
    "    Image_no_ ='{:05d}'.format(case_dict['Image_no'])\n",
    "    patient_no_ = '{:04d}'.format(case_dict['Patient_no'])\n",
    "    case_no_ ='{:05d}'.format(case_dict['Case_no'])\n",
    "    fname_ = f'{task_}_{center_}_{Image_no_}'\n",
    "    fname_1= f'{task_}_{center_}'\n",
    "    PROCEDURE_ = case_dict['Procedure']\n",
    "    SEX_ = case_dict['Sex']\n",
    "    AGE_ = int(case_dict['Age'])\n",
    "    LOCATION1_ = case_dict['Location1']\n",
    "    LOCATION2_ = case_dict['Location2']\n",
    "    INVASIVENESS_= case_dict['Invasiveness']\n",
    "    LN_meta_= case_dict['LN meta']\n",
    "    Distant_meta_ = case_dict['Distant meta']  \n",
    "    PATHOLOGIST_ = '{:02d}'.format(case_dict['Pathologist'])\n",
    "    Clinical_information = OrderedDict(HOSP=center_, PATIENT_NO=patient_no_,CASE_NO=case_no_, AGE=AGE_,SEX=SEX_,\n",
    "                                       PROCEDURE=PROCEDURE_,LOCATION1=LOCATION1_,LOCATION2=LOCATION2_,INVASIVENESS=INVASIVENESS_,\n",
    "                                       LN_meta=LN_meta_,Distant_meta=Distant_meta_,PATHOLOGIST=PATHOLOGIST_)\n",
    "\n",
    "    xml_path_ = os.path.join(base_path,'06',fname_1,fname_,fname_+'.xml')\n",
    "    json_path_ = os.path.join(base_path,'06',fname_1,fname_,fname_+'.json')\n",
    "    if json_path in \n",
    "    print(xml_path_)\n",
    "    \n",
    "    try:\n",
    "        if os.path.isfile(xml_path_):\n",
    "            tree = ET.parse(xml_path_)\n",
    "            root = tree.getroot()\n",
    "            Layer1 = []\n",
    "            Layer2 = []\n",
    "            # aperio image scope\n",
    "            if len(root.attrib):\n",
    "                # layer 1 : 대표 검체, layer2 : 병변 \n",
    "                assert len(list(root.iter('Annotation'))) == 2,f\"{fname_}.xml does not match the ID range, len {len(list(root.iter('Annotation')))}\"\n",
    "                for root_ in (root.iter('Annotation')):\n",
    "                    for anno_ in (root_.iter('Region')):\n",
    "                        rect_ = []\n",
    "                        for child in anno_.iter(\"Vertex\"):\n",
    "                            coodinates = child.attrib\n",
    "                            x_ = int(round(float(coodinates.get('X'))))\n",
    "                            y_ = int(round(float(coodinates.get('Y'))))\n",
    "                            rect_.append({\"X\": x_, \"Y\":y_})\n",
    "                        if root_.attrib['Id'] == '1':\n",
    "                            Layer1.append({'Region' : rect_,'NegativeROA' : int(anno_.attrib['NegativeROA'])})\n",
    "                        elif root_.attrib['Id'] == '2':\n",
    "                            Layer2.append({'Region' : rect_,'NegativeROA' : int(anno_.attrib['NegativeROA'])})\n",
    "                        else:\n",
    "                            raise Exception(f'{fname_} is out of ID format')\n",
    "            # ASAP\n",
    "            else:\n",
    "                for root_ in root.iter('Annotation'):\n",
    "                    g_n = root_.attrib['PartOfGroup']\n",
    "                    if g_n[-1] == 'P':\n",
    "                        nega_pen = 0\n",
    "                    elif g_n[-1] == 'N':\n",
    "                        nega_pen = 1\n",
    "                    else:\n",
    "                        raise Exception\n",
    "\n",
    "                    rect_ = []\n",
    "                    for anno in root_.iter('Coordinate'):\n",
    "                        coodinates = anno.attrib\n",
    "                        x_ = int(round(float(coodinates.get('X'))))\n",
    "                        y_ = int(round(float(coodinates.get('Y'))))\n",
    "                        rect_.append({\"X\": x_, \"Y\":y_})\n",
    "                    if g_n[:2] =='L1':\n",
    "                        Layer1.append({'Region' : rect_,'NegativeROA' : nega_pen})\n",
    "                    elif g_n[:2] =='L2':\n",
    "                        Layer2.append({'Region' : rect_,'NegativeROA' : nega_pen})\n",
    "                    else:\n",
    "                        raise Exception(f'{fname_} is out of ID format')\n",
    "\n",
    "            result_ = OrderedDict(Clinical_information=Clinical_information,Layer1=Layer1,Layer2=Layer2)\n",
    "    #     print(result_)\n",
    "    #     else:\n",
    "    #         raise Exception(f'No such {fname_}.xml in {base_path}')\n",
    "\n",
    "            if os.path.isdir(base_save_path):\n",
    "                save_json_path_ = f'{base_save_path}/{task_}/{task_}_{center_}/{task_}_{center_}_{Image_no_}'\n",
    "                print(save_json_path_)\n",
    "                os.makedirs(save_json_path_,exist_ok=True)\n",
    "\n",
    "                save_json_path_ = os.path.join(save_json_path_,fname_+'.json')\n",
    "                with open(save_json_path_, \"w\", encoding='UTF-8-sig') as json_:\n",
    "                    json_.write(json.dumps(result_, indent=4,sort_keys=False))  \n",
    "            else :\n",
    "                execpt_list_1.append(xml_path_)\n",
    "    except:\n",
    "        except_list.append(xml_path_)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3237fe6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "except_list=[]\n",
    "execpt_list_1=[]\n",
    "for row_ in df.iterrows():\n",
    "    case_dict = dict(row_[1])\n",
    "    case_dict = {k : None if case_dict[k] == -9999999 else v for k, v in case_dict.items()}\n",
    "    task_ = '{:02d}'.format(case_dict['Task_Name'])\n",
    "    center_ = '{:02d}'.format(case_dict['Center'])\n",
    "    Image_no_ ='{:05d}'.format(case_dict['Image_no'])\n",
    "    patient_no_ = '{:04d}'.format(case_dict['Patient_no'])\n",
    "    case_no_ ='{:05d}'.format(case_dict['Case_no'])\n",
    "    fname_ = f'{task_}_{center_}_{Image_no_}'\n",
    "    fname_1= f'{task_}_{center_}'\n",
    "    PROCEDURE_ = case_dict['Procedure']\n",
    "    SEX_ = case_dict['Sex']\n",
    "    AGE_ = int(case_dict['Age'])\n",
    "    LOCATION1_ = case_dict['Location1']\n",
    "    LOCATION2_ = case_dict['Location2']\n",
    "    INVASIVENESS_= case_dict['Invasiveness']\n",
    "    LN_meta_= case_dict['LN meta']\n",
    "    Distant_meta_ = case_dict['Distant meta']  \n",
    "    PATHOLOGIST_ = '{:02d}'.format(case_dict['Pathologist'])\n",
    "    Clinical_information = OrderedDict(HOSP=center_, PATIENT_NO=patient_no_,CASE_NO=case_no_, AGE=AGE_,SEX=SEX_,\n",
    "                                       PROCEDURE=PROCEDURE_,LOCATION1=LOCATION1_,LOCATION2=LOCATION2_,INVASIVENESS=INVASIVENESS_,\n",
    "                                       LN_meta=LN_meta_,Distant_meta=Distant_meta_,PATHOLOGIST=PATHOLOGIST_)\n",
    "\n",
    "    xml_path_ = os.path.join(base_path,'06',fname_1,fname_,fname_+'.xml')\n",
    "    print(xml_path_)\n",
    "    try:\n",
    "        if os.path.isfile(xml_path_):\n",
    "            tree = ET.parse(xml_path_)\n",
    "            root = tree.getroot()\n",
    "            Layer1 = []\n",
    "            Layer2 = []\n",
    "            # aperio image scope\n",
    "            if len(root.attrib):\n",
    "                # layer 1 : 대표 검체, layer2 : 병변 \n",
    "                assert len(list(root.iter('Annotation'))) == 2,f\"{fname_}.xml does not match the ID range, len {len(list(root.iter('Annotation')))}\"\n",
    "                for root_ in (root.iter('Annotation')):\n",
    "                    for anno_ in (root_.iter('Region')):\n",
    "                        rect_ = []\n",
    "                        for child in anno_.iter(\"Vertex\"):\n",
    "                            coodinates = child.attrib\n",
    "                            x_ = int(round(float(coodinates.get('X'))))\n",
    "                            y_ = int(round(float(coodinates.get('Y'))))\n",
    "                            rect_.append({\"X\": x_, \"Y\":y_})\n",
    "                        if root_.attrib['Id'] == '1':\n",
    "                            Layer1.append({'Region' : rect_,'NegativeROA' : int(anno_.attrib['NegativeROA'])})\n",
    "                        elif root_.attrib['Id'] == '2':\n",
    "                            Layer2.append({'Region' : rect_,'NegativeROA' : int(anno_.attrib['NegativeROA'])})\n",
    "                        else:\n",
    "                            raise Exception(f'{fname_} is out of ID format')\n",
    "            # ASAP\n",
    "            else:\n",
    "                for root_ in root.iter('Annotation'):\n",
    "                    g_n = root_.attrib['PartOfGroup']\n",
    "                    if g_n[-1] == 'P':\n",
    "                        nega_pen = 0\n",
    "                    elif g_n[-1] == 'N':\n",
    "                        nega_pen = 1\n",
    "                    else:\n",
    "                        raise Exception\n",
    "\n",
    "                    rect_ = []\n",
    "                    for anno in root_.iter('Coordinate'):\n",
    "                        coodinates = anno.attrib\n",
    "                        x_ = int(round(float(coodinates.get('X'))))\n",
    "                        y_ = int(round(float(coodinates.get('Y'))))\n",
    "                        rect_.append({\"X\": x_, \"Y\":y_})\n",
    "                    if g_n[:2] =='L1':\n",
    "                        Layer1.append({'Region' : rect_,'NegativeROA' : nega_pen})\n",
    "                    elif g_n[:2] =='L2':\n",
    "                        Layer2.append({'Region' : rect_,'NegativeROA' : nega_pen})\n",
    "                    else:\n",
    "                        raise Exception(f'{fname_} is out of ID format')\n",
    "\n",
    "            result_ = OrderedDict(Clinical_information=Clinical_information,Layer1=Layer1,Layer2=Layer2)\n",
    "    #     print(result_)\n",
    "    #     else:\n",
    "    #         raise Exception(f'No such {fname_}.xml in {base_path}')\n",
    "\n",
    "            if os.path.isdir(base_save_path):\n",
    "                save_json_path_ = f'{base_save_path}/{task_}/{task_}_{center_}/{task_}_{center_}_{Image_no_}'\n",
    "                print(save_json_path_)\n",
    "                os.makedirs(save_json_path_,exist_ok=True)\n",
    "\n",
    "                save_json_path_ = os.path.join(save_json_path_,fname_+'.json')\n",
    "                with open(save_json_path_, \"w\", encoding='UTF-8-sig') as json_:\n",
    "                    json_.write(json.dumps(result_, indent=4,sort_keys=False))  \n",
    "            else :\n",
    "                execpt_list_1.append(xml_path_)\n",
    "    except:\n",
    "        except_list.append(xml_path_)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f06f04d",
   "metadata": {},
   "source": [
    "## json parsing except_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4eed9c18",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/workspace/task6/04_최종_xml/06/06_01/06_01_00341/06_01_00341.xml',\n",
       " '/workspace/task6/04_최종_xml/06/06_01/06_01_00734/06_01_00734.xml',\n",
       " '/workspace/task6/04_최종_xml/06/06_02/06_02_00168/06_02_00168.xml',\n",
       " '/workspace/task6/04_최종_xml/06/06_02/06_02_00233/06_02_00233.xml',\n",
       " '/workspace/task6/04_최종_xml/06/06_02/06_02_00235/06_02_00235.xml',\n",
       " '/workspace/task6/04_최종_xml/06/06_03/06_03_00073/06_03_00073.xml',\n",
       " '/workspace/task6/04_최종_xml/06/06_03/06_03_00076/06_03_00076.xml',\n",
       " '/workspace/task6/04_최종_xml/06/06_03/06_03_00079/06_03_00079.xml',\n",
       " '/workspace/task6/04_최종_xml/06/06_03/06_03_00126/06_03_00126.xml',\n",
       " '/workspace/task6/04_최종_xml/06/06_03/06_03_00209/06_03_00209.xml',\n",
       " '/workspace/task6/04_최종_xml/06/06_04/06_04_00045/06_04_00045.xml',\n",
       " '/workspace/task6/04_최종_xml/06/06_04/06_04_00572/06_04_00572.xml',\n",
       " '/workspace/task6/04_최종_xml/06/06_04/06_04_00867/06_04_00867.xml',\n",
       " '/workspace/task6/04_최종_xml/06/06_04/06_04_00868/06_04_00868.xml',\n",
       " '/workspace/task6/04_최종_xml/06/06_04/06_04_01074/06_04_01074.xml']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "except_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09756131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('Clinical_information',\n",
       "              OrderedDict([('HOSP', '04'),\n",
       "                           ('PATIENT_NO', '0540'),\n",
       "                           ('CASE_NO', '00001'),\n",
       "                           ('AGE', 48),\n",
       "                           ('SEX', 'F'),\n",
       "                           ('PROCEDURE', 'Biopsy'),\n",
       "                           ('LOCATION1', 'EXTREMITY'),\n",
       "                           ('LOCATION2', 'Sole'),\n",
       "                           ('INVASIVENESS', 'In situ'),\n",
       "                           ('LN_meta', 'NS'),\n",
       "                           ('Distant_meta', 'No'),\n",
       "                           ('PATHOLOGIST', '11')])),\n",
       "             ('Layer1',\n",
       "              [{'Region': [{'X': 34887, 'Y': 96082},\n",
       "                 {'X': 27261, 'Y': 96463},\n",
       "                 {'X': 22114, 'Y': 102754},\n",
       "                 {'X': 20970, 'Y': 110951},\n",
       "                 {'X': 20970, 'Y': 118768},\n",
       "                 {'X': 24783, 'Y': 125440},\n",
       "                 {'X': 32409, 'Y': 126965},\n",
       "                 {'X': 39271, 'Y': 122962},\n",
       "                 {'X': 41178, 'Y': 115527},\n",
       "                 {'X': 41559, 'Y': 107901},\n",
       "                 {'X': 40987, 'Y': 100085}],\n",
       "                'NegativeROA': 0}]),\n",
       "             ('Layer2', [])])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f27194d",
   "metadata": {},
   "source": [
    "## schema validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ebb6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jsonschema import validate\n",
    "from pathlib import Path\n",
    "import json\n",
    "import tqdm\n",
    "from glob import glob\n",
    "insitu_except_list=[]\n",
    "in_path_list=[]\n",
    "TASK_NUM = \"06\"\n",
    "BASE_PATH = \"/workspace/task6/04_최종_라벨링데이터\"\n",
    "jsonfile_path_list = glob(f\"{BASE_PATH}/{TASK_NUM}/*/*/*.json\")\n",
    "jsonfile_path_list\n",
    "schema_path = f\"/workspace/task6/task{TASK_NUM}_schema.json\"\n",
    "with open (schema_path, \"r\", encoding=\"utf-8-sig\") as schemafile:\n",
    "    schema_data = json.load(schemafile)\n",
    "    \n",
    "for path in tqdm.tqdm(jsonfile_path_list):\n",
    "    with open (path, \"r\", encoding=\"utf-8-sig\") as jsonfile:\n",
    "        json_data = json.load(jsonfile)\n",
    "    try :\n",
    "        validate(instance=json_data, schema=schema_data)\n",
    "        in_path_list.append(path)\n",
    "    except Exception as e :\n",
    "        insitu_except_list.append(path)\n",
    "#         damn.append(e.message)\n",
    "#         assholes.append(jsonfile)\n",
    "        name_ = path.split('/')[-1].replace('.json','')\n",
    "        print(f\"{name_} : {e.message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7708efc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "951"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(insitu_except_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "09beb5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df.loc[df['Invasiveness'] == 'Invasive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "33743a17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Task_Name</th>\n",
       "      <th>Center</th>\n",
       "      <th>Image_no</th>\n",
       "      <th>Patient_no</th>\n",
       "      <th>Case_no</th>\n",
       "      <th>Slide_no</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Procedure</th>\n",
       "      <th>Location1</th>\n",
       "      <th>Location2</th>\n",
       "      <th>Invasiveness</th>\n",
       "      <th>LN meta</th>\n",
       "      <th>Distant meta</th>\n",
       "      <th>Pathologist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>F</td>\n",
       "      <td>Resection</td>\n",
       "      <td>HEAD_NECK</td>\n",
       "      <td>Face</td>\n",
       "      <td>Invasive</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>F</td>\n",
       "      <td>Resection</td>\n",
       "      <td>HEAD_NECK</td>\n",
       "      <td>Face</td>\n",
       "      <td>Invasive</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>F</td>\n",
       "      <td>Resection</td>\n",
       "      <td>HEAD_NECK</td>\n",
       "      <td>Face</td>\n",
       "      <td>Invasive</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>F</td>\n",
       "      <td>Resection</td>\n",
       "      <td>HEAD_NECK</td>\n",
       "      <td>Face</td>\n",
       "      <td>Invasive</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>F</td>\n",
       "      <td>Resection</td>\n",
       "      <td>HEAD_NECK</td>\n",
       "      <td>Face</td>\n",
       "      <td>Invasive</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3059</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1080</td>\n",
       "      <td>466</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>F</td>\n",
       "      <td>Resection</td>\n",
       "      <td>EXTREMITY</td>\n",
       "      <td>NS</td>\n",
       "      <td>Invasive</td>\n",
       "      <td>NS</td>\n",
       "      <td>No</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3060</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1081</td>\n",
       "      <td>467</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>M</td>\n",
       "      <td>Biopsy</td>\n",
       "      <td>EXTREMITY</td>\n",
       "      <td>Lower leg</td>\n",
       "      <td>Invasive</td>\n",
       "      <td>NS</td>\n",
       "      <td>No</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3061</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1082</td>\n",
       "      <td>468</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>F</td>\n",
       "      <td>Biopsy</td>\n",
       "      <td>EXTREMITY</td>\n",
       "      <td>Foot dorsum</td>\n",
       "      <td>Invasive</td>\n",
       "      <td>NS</td>\n",
       "      <td>No</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3062</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1083</td>\n",
       "      <td>469</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>M</td>\n",
       "      <td>Biopsy</td>\n",
       "      <td>EXTREMITY</td>\n",
       "      <td>Sole</td>\n",
       "      <td>Invasive</td>\n",
       "      <td>NS</td>\n",
       "      <td>No</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3063</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1084</td>\n",
       "      <td>470</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>M</td>\n",
       "      <td>Resection</td>\n",
       "      <td>TRUNK</td>\n",
       "      <td>Inguinal</td>\n",
       "      <td>Invasive</td>\n",
       "      <td>NS</td>\n",
       "      <td>Yes</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>310 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Task_Name  Center  Image_no  Patient_no  Case_no  Slide_no  Age Sex  \\\n",
       "0             6       1         1           1        1         1   60   F   \n",
       "1             6       1         2           1        1         2   60   F   \n",
       "2             6       1         3           1        1         3   60   F   \n",
       "3             6       1         4           1        1         4   60   F   \n",
       "4             6       1         5           1        1         5   60   F   \n",
       "...         ...     ...       ...         ...      ...       ...  ...  ..   \n",
       "3059          6       4      1080         466        1         1   48   F   \n",
       "3060          6       4      1081         467        1         1   34   M   \n",
       "3061          6       4      1082         468        1         1   37   F   \n",
       "3062          6       4      1083         469        1         1   66   M   \n",
       "3063          6       4      1084         470        1         1   61   M   \n",
       "\n",
       "      Procedure  Location1    Location2 Invasiveness LN meta Distant meta  \\\n",
       "0     Resection  HEAD_NECK         Face     Invasive     Yes           No   \n",
       "1     Resection  HEAD_NECK         Face     Invasive     Yes           No   \n",
       "2     Resection  HEAD_NECK         Face     Invasive     Yes           No   \n",
       "3     Resection  HEAD_NECK         Face     Invasive     Yes           No   \n",
       "4     Resection  HEAD_NECK         Face     Invasive     Yes           No   \n",
       "...         ...        ...          ...          ...     ...          ...   \n",
       "3059  Resection  EXTREMITY           NS     Invasive      NS           No   \n",
       "3060     Biopsy  EXTREMITY    Lower leg     Invasive      NS           No   \n",
       "3061     Biopsy  EXTREMITY  Foot dorsum     Invasive      NS           No   \n",
       "3062     Biopsy  EXTREMITY         Sole     Invasive      NS           No   \n",
       "3063  Resection      TRUNK     Inguinal     Invasive      NS          Yes   \n",
       "\n",
       "      Pathologist  \n",
       "0              11  \n",
       "1              11  \n",
       "2              11  \n",
       "3              11  \n",
       "4              11  \n",
       "...           ...  \n",
       "3059           11  \n",
       "3060           11  \n",
       "3061           11  \n",
       "3062           11  \n",
       "3063           11  \n",
       "\n",
       "[310 rows x 15 columns]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_1209"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf399158",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/workspace/task6/04_최종_라벨링데이터'\n",
    "invasive_test_path=[]\n",
    "for row_ in tmp_1209.iterrows():\n",
    "    case_dict = dict(row_[1])\n",
    "    case_dict = {k : None if case_dict[k] == -9999999 else v for k, v in case_dict.items()}\n",
    "    task_ = '{:02d}'.format(case_dict['Task_Name'])\n",
    "    center_ = '{:02d}'.format(case_dict['Center'])\n",
    "    Image_no_ ='{:05d}'.format(case_dict['Image_no'])\n",
    "    patient_no_ = '{:04d}'.format(case_dict['Patient_no'])\n",
    "    case_no_ ='{:05d}'.format(case_dict['Case_no'])\n",
    "    fname_ = f'{task_}_{center_}_{Image_no_}'\n",
    "    fname_1= f'{task_}_{center_}'\n",
    "    INVASIVENESS_= case_dict['Invasiveness']\n",
    "    json_path_ = os.path.join(base_path,'06',fname_1,fname_,fname_+'.json')\n",
    "    invasive_test_path.append(json_path_)\n",
    "invasive_test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "5f7633ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(invasive_test_path).to_csv('test_final_1209')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd417b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/workspace/task6/04_최종_라벨링데이터'\n",
    "invasive_train_path=[]\n",
    "for row_ in df4.iterrows():\n",
    "    case_dict = dict(row_[1])\n",
    "    case_dict = {k : None if case_dict[k] == -9999999 else v for k, v in case_dict.items()}\n",
    "    task_ = '{:02d}'.format(case_dict['Task_Name'])\n",
    "    center_ = '{:02d}'.format(case_dict['Center'])\n",
    "    Image_no_ ='{:05d}'.format(case_dict['Image_no'])\n",
    "    patient_no_ = '{:04d}'.format(case_dict['Patient_no'])\n",
    "    case_no_ ='{:05d}'.format(case_dict['Case_no'])\n",
    "    fname_ = f'{task_}_{center_}_{Image_no_}'\n",
    "    fname_1= f'{task_}_{center_}'\n",
    "    INVASIVENESS_= case_dict['Invasiveness']\n",
    "    json_path_ = os.path.join(base_path,'06',fname_1,fname_,fname_+'.json')\n",
    "    invasive_train_path.append(json_path_)\n",
    "invasive_train_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdc5205",
   "metadata": {},
   "source": [
    "## patch extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b173c16b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import openslide\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "LEVEL_IDX ={\n",
    "    'level0' : 0,\n",
    "    'level2' : 1,\n",
    "    'level4' : 2,\n",
    "}\n",
    "LEVEL_RESOLUTION = {\n",
    "    'level0' : 1.0,\n",
    "    'level1' : 2.0, # 1/2 resoltion\n",
    "    'level2' : 4.0, # 1/4 resolution\n",
    "    'level3' : 8.0, # 1/4 resolution\n",
    "    'level4' : 16.0, # 1/16 resolution\n",
    "}\n",
    "PATCH_SIZE=  512\n",
    "EXTRACTION_LEVEL = 'level2'\n",
    "svs_base_path = '/workspace/task6/'\n",
    "save_base_path = '/workspace/task6/04_final_patch'\n",
    "# ''/workspace/task6/03_중간단계/06/06_02/06_02_00363/06_02_00363'\n",
    "\n",
    "json_path_list=invasive_train_path\n",
    "# json_path_list = in_path_list  ##schema validation 완료된 파일만 불러오기\n",
    "# print(len(json_path_list))\n",
    "name_list = []\n",
    "except_list = []\n",
    "except_list_json= []\n",
    "df_ex=pd.DataFrame(columns=['slide name','N patches'])\n",
    "for path_ in tqdm(sorted(json_path_list[13:])):\n",
    "    name_ = path_.split('/')[-1].replace('.json','')\n",
    "    name_list.append(name_)\n",
    "    #slide setting\n",
    "    # 이부분 수정해야됨\n",
    "    base_slide_path = path_.replace('04_최종_라벨링데이터','04_원천데이터').replace('.json','')\n",
    "    \n",
    "#     is_svs = False\n",
    "    try:\n",
    "        if os.path.isfile(base_slide_path+'.svs') == True:           \n",
    "            wsi_slide = openslide.OpenSlide(base_slide_path+'.svs')           \n",
    "            mpp = round(float(wsi_slide.properties['aperio.MPP']),2)\n",
    "        elif os.path.isfile(base_slide_path+'.mrxs') == True:\n",
    "            wsi_slide = openslide.OpenSlide(base_slide_path+'.mrxs')\n",
    "            mpp = round(float(wsi_slide.properties['openslide.mpp-x']),2)\n",
    "        elif os.path.isfile(base_slide_path+'.tif') == True:\n",
    "            wsi_slide = openslide.OpenSlide(base_slide_path+'.tif')\n",
    "            mpp = round(float(wsi_slide.properties['openslide.comment'].split('MPP=')[-1]),2)        \n",
    "        else:\n",
    "            raise Exception('{} 해당 경로의 파일 없음.'.format(base_slide_path))\n",
    "        print(base_slide_path)\n",
    "    except:        \n",
    "        except_list.append(base_slide_path)\n",
    "        print('ex',base_slide_path)\n",
    "        continue\n",
    "    level_list = [round(i) for i in wsi_slide.level_downsamples]\n",
    "    level_idx = level_list.index(LEVEL_RESOLUTION[EXTRACTION_LEVEL])\n",
    "    \n",
    "#     if os.path.isfile(svs_path) == False:\n",
    "#         raise Exception('{} 해당 경로의 파일 없음.'.format(svs_path))\n",
    "#     wsi_slide = openslide.OpenSlide(svs_path)\n",
    "    #check mpp\n",
    "    try:\n",
    "        if mpp > 0.40:\n",
    "            wsi_w,wsi_h = wsi_slide.level_dimensions[level_idx]\n",
    "            wsi_array = np.array(wsi_slide.read_region([0,0], level_idx, [wsi_w, wsi_h]))[:,:,:3]\n",
    "            MPP_W = 1\n",
    "            print(wsi_array.shape)\n",
    "        else:\n",
    "            wsi_w,wsi_h = wsi_slide.level_dimensions[level_idx]\n",
    "            wsi_array = np.array(wsi_slide.read_region([0,0], level_idx, [wsi_w, wsi_h]))[:,:,:3]\n",
    "            wsi_array = cv2.resize(wsi_array, dsize=(0,0),fx =0.5,fy = 0.5, interpolation=cv2.INTER_AREA)\n",
    "            wsi_h,wsi_w = wsi_array.shape[:-1]\n",
    "            print(wsi_h,wsi_w)\n",
    "            MPP_W = 2\n",
    "    except:\n",
    "        except_list.append(base_slide_path)\n",
    "        print('ex',base_slide_path)\n",
    "        continue  \n",
    "        \n",
    "        #json setting\n",
    "    try:\n",
    "        with open (path_, \"r\", encoding=\"utf-8-sig\") as jsonfile:\n",
    "            dict_ = json.load(jsonfile)\n",
    "            layer_1= dict_['Layer1']\n",
    "            layer_2= dict_['Layer2']\n",
    "    except:\n",
    "        except_list_json.append(base_slide_path)\n",
    "        print('ex',base_slide_path)\n",
    "        continue         \n",
    "    #layer1\n",
    "    layer1_mat_posi = np.zeros((wsi_h,wsi_w))\n",
    "    layer1_mat_nega = np.zeros((wsi_h,wsi_w))\n",
    "    \n",
    "    #foreground만 추출\n",
    "    wsi_array_gray = cv2.cvtColor(wsi_array,cv2.COLOR_RGB2GRAY)\n",
    "    th_, otsu_result =  cv2.threshold(wsi_array_gray, 240, 255, cv2.THRESH_BINARY_INV) \n",
    "    kernel = np.ones((25,25),np.uint8)\n",
    "    foreground = cv2.morphologyEx(otsu_result, cv2.MORPH_CLOSE, kernel)\n",
    "    for region_ in layer_1:\n",
    "        bound_ = []\n",
    "        for cord_ in region_['Region']:\n",
    "            x_,y_ = cord_['X']//int(LEVEL_RESOLUTION[EXTRACTION_LEVEL]*MPP_W), cord_['Y']//int(LEVEL_RESOLUTION[EXTRACTION_LEVEL]*MPP_W)\n",
    "            bound_.append([x_,y_])\n",
    "        cnt_ = np.array(bound_).reshape((-1, 1, 2)).astype(np.int32)\n",
    "        if region_['NegativeROA']== 0:\n",
    "                cv2.fillPoly(layer1_mat_posi, [cnt_], 1)\n",
    "        elif region_['NegativeROA']== 1:\n",
    "            cv2.fillPoly(layer1_mat_nega, [cnt_], 1)\n",
    "    layer1_mat_posi[foreground==0] = 0\n",
    "    layer1_mat_posi[layer1_mat_nega==1] = 0\n",
    "    #layer2\n",
    "    layer2_mat_posi = np.zeros((wsi_h,wsi_w))\n",
    "    layer2_mat_nega = np.zeros((wsi_h,wsi_w))\n",
    "    for region_ in layer_2:\n",
    "        bound_ = []\n",
    "        for cord_ in region_['Region']:\n",
    "            x_,y_ = cord_['X']//int(LEVEL_RESOLUTION[EXTRACTION_LEVEL]*MPP_W), cord_['Y']//int(LEVEL_RESOLUTION[EXTRACTION_LEVEL]*MPP_W)\n",
    "            bound_.append([x_,y_])\n",
    "        cnt_ = np.array(bound_).reshape((-1, 1, 2)).astype(np.int32)\n",
    "        if region_['NegativeROA']== 0:\n",
    "                cv2.fillPoly(layer2_mat_posi, [cnt_], 1)\n",
    "        elif region_['NegativeROA']== 1:\n",
    "            cv2.fillPoly(layer2_mat_nega, [cnt_], 1)\n",
    "    layer2_mat_posi[layer2_mat_nega==1] = 0\n",
    "    if np.sum(layer1_mat_posi)== 0 :\n",
    "        continue\n",
    "                \n",
    "#102911:23             \n",
    "#     #json setting -> 좌표 추출\n",
    "#     with open (path_, \"r\", encoding=\"utf-8-sig\") as jsonfile:\n",
    "#         dict_ = json.load(jsonfile)\n",
    "#         layer_1= dict_['Layer1']\n",
    "#         layer_2= dict_['Layer2']\n",
    "#     #layer1\n",
    "#     layer1_mat_posi = np.zeros((wsi_h,wsi_w))\n",
    "#     layer1_mat_nega = np.zeros((wsi_h,wsi_w))\n",
    "#     for region_ in layer_1:\n",
    "#         bound_ = []\n",
    "#         for cord_ in region_['Region']:\n",
    "#             x_,y_ = cord_['X']//int(4.0*MPP_W), cord_['Y']//int(4.0*MPP_W)\n",
    "#             bound_.append([x_,y_])\n",
    "#         cnt_ = np.array(bound_).reshape((-1, 1, 2)).astype(np.int32)\n",
    "#         if region_['NegativeROA']== 0:\n",
    "#                 cv2.fillPoly(layer1_mat_posi, [cnt_], 1)\n",
    "#         elif region_['NegativeROA']== 1:\n",
    "#             cv2.fillPoly(layer1_mat_nega, [cnt_], 1)\n",
    "#     layer1_mat_posi[layer1_mat_nega==1] = 0\n",
    "#     #layer2\n",
    "#     layer2_mat_posi = np.zeros((wsi_h,wsi_w))\n",
    "#     layer2_mat_nega = np.zeros((wsi_h,wsi_w))\n",
    "#     for region_ in layer_2:\n",
    "#         bound_ = []\n",
    "#         for cord_ in region_['Region']:\n",
    "#          #x,y는 level0에서 그려지므로, 위에서 우리는 img를 resize했으므로, 좌표도 똑같이 나눠준다.\n",
    "#             x_,y_ = cord_['X']//int(4.0*MPP_W), cord_['Y']//int(4.0*MPP_W)\n",
    "#             bound_.append([x_,y_])\n",
    "#         cnt_ = np.array(bound_).reshape((-1, 1, 2)).astype(np.int32)\n",
    "#         #positive.영역 안 그리기\n",
    "#         if region_['NegativeROA']== 0:\n",
    "#                 cv2.fillPoly(layer2_mat_posi, [cnt_], 1)\n",
    "#         #negagive 영역 안 그리기\n",
    "#         elif region_['NegativeROA']== 1:\n",
    "#             cv2.fillPoly(layer2_mat_nega, [cnt_], 1)\n",
    "# # #     if is_svs ==True:\n",
    "# #     plt.imshow(layer1_mat_posi)\n",
    "# #     plt.show()\n",
    "# #     plt.imshow(layer1_mat_nega)\n",
    "# #     plt.show()\n",
    "# #     plt.imshow(layer2_mat_posi)\n",
    "# #     plt.show()\n",
    "# #     plt.imshow(layer2_mat_nega)\n",
    "# #     plt.show()\n",
    "# #     else:\n",
    "# #         continue\n",
    "# #     print(np.unique(layer1_mat_posi))\n",
    "    count=0\n",
    "    print(wsi_h)\n",
    "    for y0 in range(0,wsi_h,PATCH_SIZE*3):\n",
    "        for x0 in range(0,wsi_w,PATCH_SIZE*3):         \n",
    "            if not ((y0+PATCH_SIZE) > wsi_h) or ((x0+PATCH_SIZE) > wsi_w):\n",
    "                core_ = np.uint8(layer1_mat_posi[y0:y0+PATCH_SIZE,x0:x0+PATCH_SIZE])\n",
    "                mask_ = np.uint8(layer2_mat_posi[y0:y0+PATCH_SIZE,x0:x0+PATCH_SIZE])\n",
    "                patch_ = np.uint8(wsi_array[y0:y0+PATCH_SIZE,x0:x0+PATCH_SIZE])\n",
    "\n",
    "                if np.sum(core_)/(PATCH_SIZE**2) > 0.7:\n",
    "                    cv2.imwrite(os.path.join(save_base_path,'level2_patch_1205',f'{name_}_X0_{int(x0*4.0*MPP_W)}_Y0_{int(y0*4.0*MPP_W)}.png'),cv2.cvtColor(patch_,cv2.COLOR_RGB2BGR))\n",
    "                    cv2.imwrite(os.path.join(save_base_path,'level2_mask_1205',f'{name_}_X0_{int(x0*4.0*MPP_W)}_Y0_{int(y0*4.0*MPP_W)}.png'),mask_)\n",
    "                    count+=1\n",
    "            else:\n",
    "                continue\n",
    "    df_ex=df_ex.append(pd.DataFrame([[path_.split('/')[-2],count]],columns=['slide name','N patches']))\n",
    "    print(count)\n",
    "name_df = pd.DataFrame(name_list,columns= ['name'])\n",
    "name_df.to_csv('tvt_df_1206_invasive_1.csv',index=False)\n",
    "\n",
    "#     plt.imshow(layer1_mat_posi)\n",
    "#     plt.show()\n",
    "#     plt.imshow(layer2_mat_posi)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e48ae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex.to_csv('invasive_Npatches.csv') # slide별 patch수 저장\n",
    "tmp_1=glob('/workspace/task6/04_final_patch/level2_patch_1205/*')\n",
    "print('total_patches:',len(tmp_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda056e1",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "3dff3cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "slide_tvt=pd.concat([e['name'],e['tvt'].sample(frac=1).reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4a1a9e10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% 2479/2479 [01:33<00:00, 26.50it/s]\n",
      "100% 301/301 [00:11<00:00, 27.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34696\n",
      "3890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# tvt_df = pd.read_csv('/workspace/task6/tvt_df.csv')\n",
    "tvt_df=df\n",
    "base_path = '/workspace/task6/04_final_patch/level2_patch_1205'\n",
    "DEVICE = 'cuda'\n",
    "BATCH_SIZE = [16,1]\n",
    "best_loss = 1000\n",
    "ls_cnt = 0\n",
    "epochs = 1\n",
    "LR = 0.00001\n",
    "def get_path_list(tvt_df,key,base_path):\n",
    "    name_list = tvt_df.loc[tvt_df['tvt']==key,'name'].tolist()\n",
    "    path_list = []\n",
    "    for name_ in tqdm(name_list):\n",
    "        path_list += glob(os.path.join(base_path,name_+'*.png'))\n",
    "    return path_list\n",
    "\n",
    "train_path_list = get_path_list(slide_tvt,'train',base_path)\n",
    "valid_path_list = get_path_list(slide_tvt,'valid',base_path)\n",
    "# test_path_list = get_path_list(slide_tvt,'test',base_path)\n",
    "print(len(train_path_list))\n",
    "print(len(valid_path_list))\n",
    "# print(len(test_path_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6203ccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_patch_list='/workspace/task6/04_final_patch/level2_patch_1205/*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6edddd02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38107"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_=glob(base_patch_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f2f95b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import os\n",
    "from torch.nn import functional as F\n",
    "import albumentations as A\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import parser\n",
    "from torchvision import transforms, utils\n",
    "import cv2\n",
    "from torch.nn import Sequential\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import pandas as pd\n",
    "from  tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import numpy as np\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "7cd65eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmentation():\n",
    "    _transform = [\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "    ]\n",
    "    return A.Compose(_transform)\n",
    "\n",
    "def get_preprocessing():\n",
    "    _transform = [\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    "    return A.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "01b2ab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(nn.Module):\n",
    "    \n",
    "    def __init__(self, d, augmentation=None, preprocessing=None):\n",
    "\n",
    "        \n",
    "        self.image_path_li = d\n",
    "        \n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = cv2.imread(self.image_path_li[idx])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  \n",
    "        \n",
    "        mask = cv2.imread(self.image_path_li[idx].replace('level2_patch_1205','level2_mask_1205'),0)\n",
    "        \n",
    "        if self.augmentation:\n",
    "                sample = self.augmentation(image=img, mask=mask)\n",
    "                img, mask = sample['image'], sample['mask']\n",
    "\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=img, mask=mask)\n",
    "            img, mask = sample['image'], sample['mask']\n",
    "              \n",
    "        return img, mask.unsqueeze(0)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_path_li) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e212869c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset(train_path_list,augmentation = get_augmentation(),preprocessing = get_preprocessing())\n",
    "valid_dataset = dataset(valid_path_list,preprocessing = get_preprocessing())\n",
    "# test_dataset = dataset(test_,preprocessing = get_preprocessing())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE[0], shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE[1], shuffle=False, num_workers=0)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "da2acb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b7-dcc49843.pth\" to /home/tester/.cache/torch/hub/checkpoints/efficientnet-b7-dcc49843.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a1c106603d4eecb770d3f116d9df36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/254M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seg_model = smp.Unet(\n",
    "    encoder_name = 'efficientnet-b7',        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\", # use `imagenet` pretrained weights for encoder initialization\n",
    "    activation = 'sigmoid',\n",
    "    in_channels=3,                  # model input channels (1 for grayscale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "301b0f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = smp.utils.losses.DiceLoss()\n",
    "metrics = [\n",
    "    smp.utils.metrics.IoU(threshold=0.5),\n",
    "    smp.utils.metrics.Fscore(threshold=0.5),\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=seg_model.parameters(), lr=LR),\n",
    "])\n",
    "\n",
    "\n",
    "# create epoch runners \n",
    "# it is a simple loop of iterating over dataloader`s samples\n",
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    seg_model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    seg_model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "20c52804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec  7 07:27:00 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.23.04    Driver Version: 455.23.04    CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 3090    On   | 00000000:01:00.0 Off |                  N/A |\n",
      "| 30%   37C    P8    20W / 350W |  24196MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 3090    On   | 00000000:25:00.0 Off |                  N/A |\n",
      "| 55%   57C    P2   192W / 350W |   7767MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce RTX 3090    On   | 00000000:41:00.0 Off |                  N/A |\n",
      "| 30%   31C    P8    20W / 350W |   1530MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce RTX 3090    On   | 00000000:61:00.0 Off |                  N/A |\n",
      "| 30%   29C    P8    16W / 350W |   1580MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  GeForce RTX 3090    On   | 00000000:81:00.0 Off |                  N/A |\n",
      "| 30%   30C    P8    17W / 350W |   3689MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  GeForce RTX 3090    On   | 00000000:A1:00.0 Off |                  N/A |\n",
      "| 30%   30C    P8    19W / 350W |  16979MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  GeForce RTX 3090    On   | 00000000:C1:00.0 Off |                  N/A |\n",
      "| 30%   30C    P8    25W / 350W |   3665MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  GeForce RTX 3090    On   | 00000000:E1:00.0 Off |                  N/A |\n",
      "| 30%   28C    P8    16W / 350W |   3713MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "f5c63176",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8196685f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_train_loss = []\n",
    "history_valid_loss = []\n",
    "history_train_iou = []\n",
    "history_valid_iou = []\n",
    "for i in range(epochs):\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    valid_logs = valid_epoch.run(valid_loader)\n",
    "    history_train_loss.append(train_logs['dice_loss'])\n",
    "    history_valid_loss.append(valid_logs['dice_loss'])\n",
    "    history_train_iou.append(train_logs['iou_score'])\n",
    "    history_valid_iou.append(valid_logs['iou_score'])\n",
    "    if best_loss > valid_logs['dice_loss']:\n",
    "        model_state = copy.deepcopy(seg_model.state_dict())\n",
    "        best_loss = valid_logs['dice_loss']\n",
    "        ls_cnt=0\n",
    "    else:\n",
    "        ls_cnt+=1\n",
    "        if ls_cnt>10:\n",
    "            torch.save({'model_state' : model_state,\n",
    "                    'history_dict': {'train_loss' : history_train_loss,'train_iou' : history_train_iou,\n",
    "                                    'valid_loss' : history_valid_loss,'valid_iou' : history_valid_iou}},\n",
    "                    f'/workspace/task6/model_final_1207/model{ls_cnt}.pth')\n",
    "        if ls_cnt ==10:\n",
    "            print('early stop')\n",
    "            break \n",
    "torch.save({'model_state' : model_state,\n",
    "            'history_dict': {'train_loss' : history_train_loss,'train_iou' : history_train_iou,\n",
    "                            'valid_loss' : history_valid_loss,'valid_iou' : history_valid_iou}},\n",
    "            'model_final_1207.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "617e3b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd model_final_1207"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
